%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}
\usepackage{graphics}
\usepackage{url}
\usepackage{amsmath}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 6.S085 Statistics for Research Projects
                        \hfill IAP 2014} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Notes by: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may be distributed
   outside this class only with the permission of the Instructor.}
   \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{4}{January 24}{Ramesh Sridharan and George Chen}{William Li}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{More Regression}

If testing $H_0: \mu = 0$ vs. $H_1: \mu \neq 0$:

\begin{itemize}
\item construct 95\% CI
\item ask if 0 is in the CI
\item if 0 not in the CI, then p-value is < 0.05
\end{itemize} 

\section{Residual Analysis}

Residual for $i$th point: $\hat{y} - y_i$

Intuitively, $\hat{y} - y_i$ versus $\hat{y}$ should look like noise if the model is good

\begin{align}
\hat{\epsilon} & = y - \hat{y} \\
& = y - X\hat{\beta} \\
& = y - [X(X^TX)^{-1}X^T]y
& = (I-H)y \\
& = (I-H)X\beta + (I-H)\epsilon \\
& = (I-H)\epsilon
\end{align}

Standardized residuals: $\frac{-\hat{\epsilon}_i}{\sqrt{1-H_{ii}}}$ 

Standarized residuals will have variance $\sigma^2$

Model says that the standardized residuals should each have variance $\sigma^2$

Fact: Under the model, $\hat{y} - y_i$ is uncorrelated with $\hat{y}_i$

\section{Outliers}

Informally: 

Outlier: point far away from the rest of the points

Leverage: How far point is from the rest of the points along the $x$ axis

Influential Point: point (typically with high leverage) that substantially affects the estimated slope $\beta_1$

Leverage for $i$th point is defined $H_{ii}$

In 1D case: $H_{ii} = \frac{x_i^2}{\sum x_j^2}$

\subsection{Influence}

Measured using Cook's distance: 

For the $i$th point: 

$D_i = \frac{1}{p \cdot \text{MSE}} \frac{H_{ii}}{(1-H_{ii})^2}\hat{\epsilon}^2$

Higher leverage means higher influence ($\frac{H_{ii}}{(1-H_{ii})^2}$)

Higher fitting error means higher influence ($\hat{\epsilon}^2$)

\section{Robust Regression}

Can we be resilient to to outliers without manually removing them beforehand?

Recall: We are trying to minimize a cost function:

$\min\limits_{\beta} \sum\limits_{i=1}^n ( Y_i - X_i \beta)^2$

$( Y_i - X_i \beta)^2$ (squared ``loss'')

As the squared loss gets bigger, the loss function will become enormous, so the 

Squared loss: $\rho(r) = r^2$


\subsection{Least absolute deviation (LAD)}

$\rho(r) = |r|$

$\min\limits_{\beta} \sum\limits_{i=1}^n ( Y_i - X_i \beta)$

Large deviations don't hurt us as much

However, not stable: a change in $x$ may dramatically impact $\beta$

\subsection{Huber Loss}

Close to the origin: squared loss

Further away: grow linearly

\subsection{Bisquare}

When your points are really far away, may not even consider them

\subsection{Probabilistic Interpretations}

Different cost functions correspond to different distributions used (for $\epsilon$)

$\epsilon \sim N(0,\sigma^2)$ corresponds to squared loss (98\% of the probability mass is within 3 standard deviations)

$\epsilon \sim \text{Laplace}$ corresponds to LAD (more probability mass further out)

\subsection{RANSAC}

Previous approaches we've talked about so far solve optimization problems

RANSAC (Random Sample Consensus) -- widely used in practice

Randomly choose two points, fit a line, find and count inliers

\begin{itemize}
\item for iteration t=1...T:
    \begin{itemize}
    \item choose random subset of points, $I_t$ as "inliers"
    \item fit model to points $I_t$
    \item find all points that are within some $\alpha$ of the model and add to $I_t$
    \item (Optional:) Refit model to points $I_t$
    \item Compute score for model
    \end{itemize}
\item Choose model with the highest score
\end{itemize}

\section{Sparse Regression}

Imagine $p$ (the number of predictors) is huge (e.g. $p \approx 10^6$)

$y = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon$

Want to figure out some small subset of predictors relevant to predicting $y$ (want most $\hat{\beta}_k$'s to be 0

Why? 
\begin{itemize}
\item It often makes the result more interpretable
\item Effective at predicting overfitting
\end{itemize}


\subsection{Ridge Regression}

$\min\limits_{\beta} = [ \sum\limits_{i=1}^n (Y_i - x_i\beta)^2 + \lambda \sum\limits_{k=1}^p \beta_k^2 ]$

Choose $\lambda > 0$

Does penalize large $\beta_k$

Note: Ridge regression will not give you a sparse solution -- it is perfectly happy if the $\beta_k$'s are close to 0

However, it is widely used for preventing overfitting

\subsection{LASSO}

What if we penalized the number of terms in the $\beta$ vector that do not equal 0?

$\min\limits_{\beta} = [ \sum\limits_{i=1}^n (Y_i - x_i\beta)^2 + \lambda \sum\limits_{k=1}^p 1[\beta_k \neq 0] ]$

Unfortunately, there is no efficient way to solve this.

However, the LASSO will give a sparse solution

$\min\limits_{\beta} = [ \sum\limits_{i=1}^n (Y_i - x_i\beta)^2 + \lambda \sum\limits_{k=1}^p |\beta_k| ]$







\section{Logistic Regression}

\end{document}





