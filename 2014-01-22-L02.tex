%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}
\usepackage{graphics}
\usepackage{url}
\usepackage{amsmath}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 6.S085 Statistics for Research Projects
                        \hfill IAP 2014} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Notes by: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may be distributed
   outside this class only with the permission of the Instructor.}
   \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{January 22}{Ramesh Sridharan and George Chen}{William Li}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.
\section{Confidence Intervals}

French baker example: 
\begin{itemize}
\item Gaussian distribution around 950g 
\item skewed distribution around 1000g
\end{itemize}

Goal: estimate the parameters of a binomial random variable

Setup: $x_1, x_2, ... , x_n$

Data points are binary

There is some true distributions (``population distribution'') with true parameters $p$

Use data to draw conclusions about $p$

How to estimate $p$?

$\hat{p} = \frac{1}{n}\sum\limits_i x_i$

$p$ isn't random, but $x_i$ are random, so $\hat{p}$ is random!

\begin{align}
E[\hat{p}] & = E[\frac{1}{n}\sum x_i] \\
& = \frac{1}{n} \sum E[x_i] \\
& = p
\end{align}

\begin{align}
var[\hat{p}] & = var[\frac{1}{n} \sum x_i] \\
& = \frac{1}{n^2} \sum var[x_i] \\
& = \frac{1}{n^2} np(1-p) \\
& = \frac{ p(1-p) }{n}
\end{align}

The variance on $\hat{p}$ goes down with more $n$.

By central limit therorem, $\hat{p}$ is approximately Gaussian: Completely characterized by mean and variance:

$\hat{p} ~ N(p, \frac{p(1-p)}{n}$

Confidence intervals: loosely, we want ``the range of intervals where $p$ probably is''

If we repeat sampling (data collection) and CI computation, 95\% of those repeats will give an interva that has $p$

Recall that, in a Gaussian, 95\% of the data lies within 2 standard deviations of the mean

$\hat{p} \pm 2 \sqrt{\frac{p(1-p)}{n}}$: This is our 95\% confidence interval

What a CI means: 

Probability of getting a $\hat{p}$ within 2 standard deviations of mean $p$

Since we don't actually know $p$, we will replace $p$ with $\hat{p}$!

$\hat{p} \pm 2 \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$: This is our 95\% confidence interval

``With probability 0.95, $\hat{p}$ will just such that this confidence intervals contains $p$

``accurate to within  $2 \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ percentage points, 95 times out of 100.''

Note that, with more $n$, we will have a tighter confidence interval

\section{Hypothesis Testing}

We have a hypothesis about $p$

Use data to see whether we can reject the hypothesis


Find out how likely the data is under the hypothesis; if it's very unlikely, reject

Null and alternative hypothesis formulation:

$H_0: p = p^*$ (null hypothesis)

$H_a: p > p^*, p < p^*, p \neq p^*$ (alternative hypothesis)

One-tailed test: $>, <$: only interested in one direction

Two-tailed test: $\neq$: interested in both or either side

Tail of Gaussian represented by $alpha$

Type I error/false positive: reject $H_0$ when $H_0$ is true

Type II error/false negative: fail to reject $H_0$ when $H_0$ is false

A hypothesis test can only tell you whether you can accept the null hypothesis or fail to accept the null hypothesis

$\alpha=0.05$: Probability of false positive

``There is only a 5\% chance that the observation happened by chance if the null observation is true''

``What is the probability of getting this value if the null hypothesis were true?''

\subsection{p values}

We have a null hypothesis value $p^*$, and then we observe $x$

We ask: ``What is the probability of getting $x$ (or something more extreme) if null hypothesis is true?''

This is what a p value is

\subsection{Statistical Power}

$1 - P(\text{Type II errror})$

Probability of rejecting $H_0$ when it's wrong

Defined with respect to a particular alternative value

Null hypothesis is centered around a particular value

The power depends on the threshold and $\alpha$

Distributions that are closer together: reduces the power of the test

``If the true value is 0.4, power is the shaded area''

\section{Continuous Hypothesis Testing, Confidence Intervals}

observe $x_1, x_2, ... , x_n$ (comes from Gaussian distribution with mean $\mu$, true variance $\sigma^2$

Start with the (unrealistic) assumption that $\sigma$ is known 

Want to estimate $\mu$

$\hat{\mu} = \frac{1}{n} \sum x_i$

$E[\hat{\mu}] = \mu$

\begin{align}
var{\hat{\mu}} & = \frac{1}{n^2} \sum var[x_i] \\
& \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n}
\end{align}

$\frac{\sigma}{\sqrt{n}}$ is the standard error fo the mean

In general, stnadard error of a statistic is the standard deviation of its sampling distribution

$Z = \frac{\hat{\mu} - \mu}{\sigma/sqrt{n}}$

$Z ~ N(0,1)$ (``test statistic'')

\subsection{Hypothesis Testing}
Hypothesis testing: $\mu$ is known; it's the null distance's mean

\subsection{Confidence Intervals}

$P( -2 \leq z \leq 2 )$

$P( -2 \leq \frac{\hat{mu} - \mu}{sigma / \sqrt{n} } \leq 2 )$

$P( \hat{\mu} - 2 \frac{\sigma}{\sqrt{n}} \leq \mu \leq \hat{\mu} + 2 \frac{\sigma}{\sqrt{n}} )$

CI: $\hat{\mu} \pm 2 \frac{\sigma}{\sqrt{n}}$

What if we don't know $\sigma$?

Use:

$\sigma^2 = \frac{1}{n-1}\sum(x_i - \hat{\mu}^2$
$\sigma^2 = s^2$

Approximate standard error: $s/\sqrt{n}$
/
$t = \frac{\hat{\mu} - \mu}{s/\sqrt{n}}$

$t$ has a t distribution with $n-1$ degrees of freedom

Now we want $P(-X \leq t \leq +X ) = 0.95$

Aside: $\frac{(n-1)}{\sigma^2} s^2 ~ \chi^2$ with n-1 degrees of freedom

\textbf{Student t distribution}

$Z ~ N(0,1)$

$\mu ~ \chi^2$ with $r$ degrees of freedom

$t = \frac{z}{\sqrt{u/r}}$ has Student t distribution


\section{Two-Sample Tests}

$x_1, ..., x_n$

$y_1, ... , y_n$

Matched pairs (correspondance), i.e. $x_1 \rightarrow y_1$, $x_2 \rightarrow y_2$

Then we can define $w_i = y_i - x_i$ run a one-sample test on $w$

\subsection{Pooled Variance}

``homoskedastic'' -- variances are the same

$s^2 = \frac{(n-1)s_x^2 + (m-1)s_y^2}{n+m-2}$

\subsection{Unpooled Variances}
;
``heteroskedastic'' -- variances are not the same

Test statistic no longer exactly t-distribution


\section{Warnings About Tests}

For hypothesis tests: 
\begin{itemize}
\item Don't say anything stronger than ``the data do not support the null hypothesis''
\item Avoid multiple comparisons, or correct (Bonferrnoni correction, divides p-value by the number of comparisons
\end{itemize}

False discovery rate (FDR)





%\section*{References}           
%\beginrefs
%\bibentry{AGM97}{\sc N.~Alon}, {\sc Z.~Galil} and {\sc O.~Margalit},
%On the Exponent of the All Pairs Shortest Path Problem,
%{\it Journal of Computer and System Sciences\/}~{\bf 54} (1997),
%pp.~255--262.

%\bibentry{F76}{\sc M. L. ~Fredman}, New Bounds on the Complexity of the 
%Shortest Path Problem, {\it SIAM Journal on Computing\/}~{\bf 5} (1976), 
%pp.~83-89.
%\endrefs


\end{document}





